{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "XDDbguyGU8Fs"
      },
      "source": [
        "# Mid-semester assignment - Advanced topics in deep learning\n",
        "Hello dear students,<br> this is the template notebook. Please upload it into your drive and open as Google Colab nootebook\".\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "### Name and ID:\n",
        "Student 1: Stav Elizur\n",
        "<br>\n",
        "Student 2: Tommy Afek"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLkWLC8f3HZI"
      },
      "source": [
        "## Coffee Bean Classification Project\n",
        "The project is a classification of images on \"coffee bean dataset resized\". It is a computer vision project that classifies the coffee beans by looking their colours. There are 4 labels that are Dark, Green, Light and Medium.\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://i.pinimg.com/564x/70/24/98/702498e0289cd18be9c160eb81357b30.jpg\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  \n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiRxNFCn3Vxd"
      },
      "source": [
        "**The Data**<br>\n",
        "The dataset is labeled into 4 different classes.\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>Dark</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Green</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Light</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Medium</td>\n",
        "  </tr>\n",
        "</table>\n",
        "<br>\n",
        "\n",
        "\n",
        "# Goodluck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4K84YZ_QU8Fv"
      },
      "source": [
        "## Part 0 -  Utiles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QReFpU112hLT"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLOHjUiFU8Fv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "!pip install -U --no-cache-dir gdown --pre\n",
        "!gdown --id 1anU5YH4vRueTEF-ZGGDEDvEY4FNfSOjF\n",
        "!unzip /content/archive.zip\n",
        "\n",
        "\n",
        "!gdown --id 1W3iNC-KXdS1txm3AB_9fYySUPoPG-WFw\n",
        "!unzip /content/Explainer.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mount to drive"
      ],
      "metadata": {
        "id": "SNBavNOFSHBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deWJ1_ltSF0n",
        "outputId": "7c59d920-972a-4940-81b1-00d68d3e8748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtR5zXzr3ezg"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5MsCpUv2tuj"
      },
      "source": [
        "### Data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKO72wSm1Ho2"
      },
      "source": [
        "How many classes do we have?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbzv9ZYA2pyW",
        "outputId": "2a3db818-1431-4451-f573-8b8b4f9bbeb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 4 different classes\n"
          ]
        }
      ],
      "source": [
        "num_of_coffee_bean_classes = len(os.listdir('/content/train'))\n",
        "print(f'There are {num_of_coffee_bean_classes} different classes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdYeCHdi1V1K"
      },
      "source": [
        "What are those classes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU_pxIYN1b88",
        "outputId": "ade8a7c5-0612-4f4b-ee64-325eabf4ae50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name of classes:\n",
            "['Dark' 'Green' 'Light' 'Medium']\n"
          ]
        }
      ],
      "source": [
        "data_dir = pathlib.Path('/content/train')\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\n",
        "print(f'Name of classes:\\n{class_names}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX40pWEv2DOF"
      },
      "source": [
        "Let's create a function to visualize images randomly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNI2-ZMk2EjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d38501af-9aee-4c68-c3fd-256346587728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "def view_random_image(target_dir, target_class):\n",
        "  target_folder = target_dir + target_class\n",
        "  random_image = random.sample(os.listdir(target_folder), 1)\n",
        "  img = mpimg.imread(target_folder + '/' + random_image[0] )\n",
        "  plt.imshow(img)\n",
        "  plt.title(target_class)\n",
        "  plt.axis('off')\n",
        "\n",
        "  print(f'Image shape: {img.shape}')\n",
        "\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8frpnLQI2l9L"
      },
      "source": [
        "Let's visualize images from all categories\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e6DJcR_2mZW",
        "outputId": "144e1841-2c56-415a-bc05-92bb551b6224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "plt.figure(figsize = (15,7))\n",
        "plt.subplot(1,4,1)\n",
        "dark_bean_image = view_random_image('/content/train/', \"Dark\")\n",
        "plt.subplot(1,4,2)\n",
        "green_bean_image = view_random_image('/content/train/', \"Green\")\n",
        "plt.subplot(1,4,3)\n",
        "light_bean_image = view_random_image('/content/train/', \"Light\")\n",
        "plt.subplot(1,4,4)\n",
        "medium_bean_image = view_random_image('/content/train/', \"Medium\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkQXkaVO3nJu"
      },
      "source": [
        "Lets's see what the csv file contains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWmvZSSl3wPG",
        "outputId": "cabd51f4-a966-4fee-a7dd-8302ccdaf414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "df = pd.read_csv('/content/Coffee Bean.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the Training set"
      ],
      "metadata": {
        "id": "D7udCL3L_6aD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_MLmU6G3HE9"
      },
      "outputs": [],
      "source": [
        "def preprocessing_training_set(batch_size):\n",
        "  from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "  train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  training_set = train_datagen.flow_from_directory('/content/train',\n",
        "                                                  target_size = (224, 224),\n",
        "                                                  batch_size = batch_size,\n",
        "                                                  class_mode = 'categorical')\n",
        "\n",
        "  return training_set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the Test set"
      ],
      "metadata": {
        "id": "_IdCsm0c_3ln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ECWXCF84GLn"
      },
      "outputs": [],
      "source": [
        "def preprocessing_test_set(batch_size):\n",
        "  from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "  test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  test_set = test_datagen.flow_from_directory('/content/test',\n",
        "                                              target_size = (224, 224),\n",
        "                                              batch_size = batch_size,\n",
        "                                              class_mode = 'categorical')\n",
        "  return test_set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Building the CNN"
      ],
      "metadata": {
        "id": "S8OCaSfl8lJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialising the CNN"
      ],
      "metadata": {
        "id": "q3g9FdJa8qMc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGG5gzLj9HMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61967596-e49f-4d67-e4bd-0d83901df7df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "cnn = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 - Convolution"
      ],
      "metadata": {
        "id": "Lf-3bfk08w-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waRKYWG5-TAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7282430e-e793-49dc-bc7b-88c06e51e298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', input_shape=[224, 224, 3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 - Pooling"
      ],
      "metadata": {
        "id": "TqrO8xLX89vN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "metadata": {
        "id": "eHFgn1vE89BO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd90294-02d3-4fbd-d7eb-a55d63cdce14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding a second convolutional layer"
      ],
      "metadata": {
        "id": "oQA3P4vx88yJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "037lM7fFLZpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a08b37-9f58-43cb-c7d3-ebaf7e9ccf70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding a third convolutional layer"
      ],
      "metadata": {
        "id": "7Yi3wNrO9HXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=48, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "metadata": {
        "id": "qYOeNQxh9FqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7506e2b3-eb77-4234-c0b6-d2c226d82205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 - Flattening"
      ],
      "metadata": {
        "id": "oFXRfK_E9KPL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l6UkQ0F-pFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0b61a8-8425-42e4-d12a-45f5e4d803bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "cnn.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4 - Fully Connected layers"
      ],
      "metadata": {
        "id": "AuYLQlEZ9QxH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnKLLOEa-rb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2efe9329-7f9f-4fd8-e63c-059b8607cdcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "cnn.add(tf.keras.layers.Dense(units=300, activation='relu'))\n",
        "cnn.add(tf.keras.layers.Dense(units=400, activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "cnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as2wUKGK9Wsv",
        "outputId": "64933af8-56a1-4d44-86de-61f11e39b0fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 - Output Layer"
      ],
      "metadata": {
        "id": "TOArrH6x9Tqu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SZ6VwGt-th8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ccfadf-9ed0-49e6-9e4d-56da03cde78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "cnn.add(tf.keras.layers.Dense(units=4, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Training the CNN"
      ],
      "metadata": {
        "id": "1WijGsls9Zq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling the CNN"
      ],
      "metadata": {
        "id": "Dpg0hn539cv8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SQrKbw5ACX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f23a55-116b-4cf5-eb97-8eb1307bbb34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ],
      "metadata": {
        "id": "Eub5Yk5C9jSP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioPRzH5SAzSV",
        "outputId": "1b744872-1d88-4e7f-cf01-61cd8e15eba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "training_set = preprocessing_training_set(40)\n",
        "test_set = preprocessing_test_set(40)\n",
        "\n",
        "history = cnn.fit(x = training_set, validation_data = test_set, epochs = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the model"
      ],
      "metadata": {
        "id": "rAEimoGkRh6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "cnn.save('/content/drive/MyDrive/Google Colab/model_cnn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK6XqY66Rg2K",
        "outputId": "27242757-6a7b-4500-95ce-853a83f69555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the model"
      ],
      "metadata": {
        "id": "TXCD1k9B3pDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --no-cache-dir gdown --pre\n",
        "!gdown --id 1G0YzxttI58tCIpUJJTRfrAx5WOlPLgmZ\n",
        "!unzip /content/model_cnn.zip\n",
        "\n",
        "cnn = tf.keras.models.load_model('/content/model_cnn')\n",
        "\n",
        "training_set = preprocessing_training_set(40)\n",
        "test_set = preprocessing_test_set(40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTbdSohmKkD5",
        "outputId": "48a5114a-c82c-4b0d-f60c-302d0883cfb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.6.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1G0YzxttI58tCIpUJJTRfrAx5WOlPLgmZ\n",
            "To: /content/model_cnn.zip\n",
            "100% 90.3M/90.3M [00:02<00:00, 30.4MB/s]\n",
            "Archive:  /content/model_cnn.zip\n",
            "   creating: model_cnn/assets/\n",
            "  inflating: model_cnn/keras_metadata.pb  \n",
            "  inflating: model_cnn/saved_model.pb  \n",
            "   creating: model_cnn/variables/\n",
            "  inflating: model_cnn/variables/variables.data-00000-of-00001  \n",
            "  inflating: model_cnn/variables/variables.index  \n",
            "Found 1200 images belonging to 4 classes.\n",
            "Found 400 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Dx1y1Zj8eDL",
        "outputId": "85d0b39a-dcf9-406b-e428-9510e64c9c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_21 (Conv2D)          (None, 222, 222, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d_21 (MaxPoolin  (None, 111, 111, 16)     0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 109, 109, 32)      4640      \n",
            "                                                                 \n",
            " max_pooling2d_22 (MaxPoolin  (None, 54, 54, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 52, 52, 48)        13872     \n",
            "                                                                 \n",
            " max_pooling2d_23 (MaxPoolin  (None, 26, 26, 48)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 32448)             0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 300)               9734700   \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 400)               120400    \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 4)                 1604      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,875,664\n",
            "Trainable params: 9,875,664\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4 - Making a single prediction"
      ],
      "metadata": {
        "id": "NbbPgySz9mVG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8lXt_60FIdG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "2da3a2b9-425c-4b0a-fdc3-b49ded6511d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b32b5ec3-4f27-4d7c-8d1f-1916fa9c9d75\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b32b5ec3-4f27-4d7c-8d1f-1916fa9c9d75\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dark (1).png to dark (1).png\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "Name of class: Dark\n",
            "{'Dark': 0, 'Green': 1, 'Light': 2, 'Medium': 3}\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "\n",
        "test_images = files.upload()\n",
        "for imageName in test_images.keys():\n",
        "  test_image = np.array(Image.open(imageName).resize((224, 224)))\n",
        "  result = cnn.predict(test_image.reshape(1,224,224,3))\n",
        "  training_set.class_indices\n",
        "  class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\n",
        "  print(f'Name of class: {class_names[np.argmax(result[0].round(), axis=-1)]}')\n",
        "  print(training_set.class_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experiments and Result"
      ],
      "metadata": {
        "id": "Wmup5CLN3bN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of the Loss training VS validation"
      ],
      "metadata": {
        "id": "XAqJZkOf98n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_training_vs_validation(history):\n",
        "  from matplotlib import pyplot as plt\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "pVc3MUPj9-Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### Visualization of the Accuracy training VS validation"
      ],
      "metadata": {
        "id": "ryK5yKhXA8pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_training_vs_validation(history):\n",
        "  from matplotlib import pyplot as plt\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "xX1LOK9R-AiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5 - Making the predictions and evaluating the model"
      ],
      "metadata": {
        "id": "KSM3Ga2ABA1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predicting the Test set results"
      ],
      "metadata": {
        "id": "BkVm0a6gqN2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predicting_test_results(cnn, test_set):\n",
        "\n",
        "  predictions = np.array([])\n",
        "  labels =  np.array([])\n",
        "\n",
        "  for x, y in test_set:\n",
        "    predictions = np.concatenate([predictions, np.argmax(cnn.predict(x),axis=1)])\n",
        "    labels = np.concatenate([labels, np.argmax(y, axis=-1)])\n",
        "    if len(labels) == 400:\n",
        "      break\n",
        "\n",
        "  return labels, predictions"
      ],
      "metadata": {
        "id": "6iZHoSaWLpsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Performance Evaluation"
      ],
      "metadata": {
        "id": "NsfpoeLXqUya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix"
      ],
      "metadata": {
        "id": "XuhXMFq0qWID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_confusion_matrix(labels, predictions):\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "\n",
        "  cm = confusion_matrix(labels, predictions)\n",
        "  print(f'Confusion Matrix:\\n\\n{cm}')\n",
        "\n",
        "  return cm"
      ],
      "metadata": {
        "id": "RZ0rj27KqZ5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix Plot"
      ],
      "metadata": {
        "id": "ppeCEcsPqcXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_confusion_matrix_as_table(labels, predictions):\n",
        "  from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "  cm = print_confusion_matrix(labels, predictions)\n",
        "  cmd = ConfusionMatrixDisplay(cm, display_labels=['Dark', 'Green', 'Light', 'Medium'])\n",
        "  cmd.plot()"
      ],
      "metadata": {
        "id": "BNGyuxMOqhS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_accuracy_score(labels, predictions):\n",
        "  from sklearn.metrics import accuracy_score\n",
        "\n",
        "  accuracy = accuracy_score(labels, predictions)\n",
        "  print(f'Accuracy: {round(accuracy*100)}%')"
      ],
      "metadata": {
        "id": "OcA-4rdQqizL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balanced Accuracy"
      ],
      "metadata": {
        "id": "UM3XRIrxrAW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_balanced_accuracy_score(labels, predictions):\n",
        "  from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "  bal_acc = balanced_accuracy_score(labels, predictions)\n",
        "  print(f'Balanced Accuracy: {round(bal_acc*100)}%')"
      ],
      "metadata": {
        "id": "UxZNB8rtrDco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Report - Precision, Recall, F1 Score and Accuracy"
      ],
      "metadata": {
        "id": "kOEgccbaBTY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_classification_report(labels, predictions):\n",
        "  from sklearn.metrics import classification_report\n",
        "\n",
        "  print(classification_report(labels, predictions, target_names=['Dark', 'Green', 'Light', 'Medium']))"
      ],
      "metadata": {
        "id": "lxtDcjx6BgxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "loss_training_vs_validation(history)\n",
        "accuracy_training_vs_validation(history)\n",
        "labels, predictions = predicting_test_results(cnn, test_set)\n",
        "print_confusion_matrix(labels, predictions)\n",
        "print_confusion_matrix_as_table(labels, predictions)\n",
        "print_accuracy_score(labels, predictions)\n",
        "print_balanced_accuracy_score(labels, predictions)\n",
        "print_classification_report(labels, predictions)"
      ],
      "metadata": {
        "id": "XaKr7MtABOUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39798d64-35a2-4ad1-8647-0536873444b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run experiments"
      ],
      "metadata": {
        "id": "Eoh-sqVr74Y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "def first_experiment(convolutional_layers_list,\n",
        "                     fully_connected_layers_list,\n",
        "                     neurons_in_layer_list,\n",
        "                     filter_size_in_layer_list):\n",
        "\n",
        "  training_set = preprocessing_training_set(40)\n",
        "  test_set = preprocessing_test_set(40)\n",
        "  index_experiment = 1\n",
        "  for convolutional_layer in convolutional_layers_list:\n",
        "    for fully_connected_layer in fully_connected_layers_list:\n",
        "      for neuron_in_layer in neurons_in_layer_list:\n",
        "        for filter_size_in_layer in filter_size_in_layer_list:\n",
        "          cnn = tf.keras.models.Sequential()\n",
        "          cnn.add(tf.keras.layers.Conv2D(filters=filter_size_in_layer, kernel_size=3, activation='relu', input_shape=[224, 224, 3]))\n",
        "          cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "          filters = filter_size_in_layer\n",
        "          for con_layer in range(convolutional_layer):\n",
        "            cnn.add(tf.keras.layers.Conv2D(filters=filters, kernel_size=3, activation='relu'))\n",
        "            cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "            filters = filters // 2\n",
        "\n",
        "          cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "          units = neuron_in_layer\n",
        "          for fully_con_layer in range(fully_connected_layer):\n",
        "            cnn.add(tf.keras.layers.Dense(units=units, activation='relu'))\n",
        "            units = units // 2\n",
        "\n",
        "          cnn.add(tf.keras.layers.Dense(units=4, activation='softmax'))\n",
        "\n",
        "          cnn.summary()\n",
        "\n",
        "          cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "          history = cnn.fit(x = training_set, validation_data = test_set, epochs = 10)\n",
        "\n",
        "          loss_training_vs_validation(history)\n",
        "          accuracy_training_vs_validation(history)\n",
        "          labels, predictions = predicting_test_results(cnn, test_set)\n",
        "          print_confusion_matrix(labels, predictions)\n",
        "          print_confusion_matrix_as_table(labels, predictions)\n",
        "          print_accuracy_score(labels, predictions)\n",
        "          print_balanced_accuracy_score(labels, predictions)\n",
        "          print_classification_report(labels, predictions)\n",
        "\n",
        "          cnn.save(f'/content/drive/MyDrive/Google Colab/Exp1/model_exp_1_cnn{index_experiment}')\n",
        "          index_experiment += 1\n",
        "first_experiment([3, 2],\n",
        "                 [3, 4],\n",
        "                 [120, 300],\n",
        "                 [32])"
      ],
      "metadata": {
        "id": "DMZHWkkxs2HV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8393b34-f245-40d7-b44e-8e555cd5a3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "def second_experiment(learning_rates,\n",
        "                      batch_sizes,\n",
        "                      numbers_of_epochs):\n",
        "\n",
        "  index_experiment = 1\n",
        "  for learning_rate in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "      training_set = preprocessing_training_set(batch_size)\n",
        "      test_set = preprocessing_test_set(batch_size)\n",
        "\n",
        "      for numbers_of_epoch in numbers_of_epochs:\n",
        "          cnn = tf.keras.models.Sequential()\n",
        "          cnn.add(tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', input_shape=[224, 224, 3]))\n",
        "          cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "          cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "          cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "          cnn.add(tf.keras.layers.Conv2D(filters=48, kernel_size=3, activation='relu'))\n",
        "          cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "          cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "          cnn.add(tf.keras.layers.Dense(units=300, activation='relu'))\n",
        "          cnn.add(tf.keras.layers.Dense(units=400, activation='relu'))\n",
        "          cnn.add(tf.keras.layers.Dense(units=4, activation='softmax'))\n",
        "\n",
        "          cnn.summary()\n",
        "\n",
        "          opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "          cnn.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "          history = cnn.fit(x = training_set, validation_data = test_set, epochs = numbers_of_epoch)\n",
        "\n",
        "          loss_training_vs_validation(history)\n",
        "          accuracy_training_vs_validation(history)\n",
        "          labels, predictions = predicting_test_results(cnn, test_set)\n",
        "          print_confusion_matrix(labels, predictions)\n",
        "          print_confusion_matrix_as_table(labels, predictions)\n",
        "          print_accuracy_score(labels, predictions)\n",
        "          print_balanced_accuracy_score(labels, predictions)\n",
        "          print_classification_report(labels, predictions)\n",
        "\n",
        "          cnn.save(f'/content/drive/MyDrive/Google Colab/Exp2/model_exp_2_cnn{index_experiment}')\n",
        "          index_experiment += 1\n",
        "          break\n",
        "\n",
        "second_experiment([0.001, 0.01, 0.1],\n",
        "                  [40, 60, 70],\n",
        "                  [5, 6, 10])"
      ],
      "metadata": {
        "id": "QbQpsbL6zydY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8312f66-a37d-43d7-b265-a107a3d49c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}